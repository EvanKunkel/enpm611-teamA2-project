# ENPM611 Project Team A2

This project contains a set of analytical tools that can be used to generate insights on issues in the [Poetry](https://github.com/python-poetry/poetry) github repository.

## Team Members

- Sumiran Jaiswal
- Evan Kunkel
- Ayushi Patel
- Pranav Raveendran

## Install dependencies

In the root directory of the application, create a virtual environment, activate that environment, and install the dependencies like so:

```
pip install -r requirements.txt
```

## Run an analysis

With everything set up, you should be able to run the existing example analysis:

```
python run.py --feature 0
```

That will output basic information about the issues to the command line and plot a bar graph showing the top 50 issue creators by number of issues they created.

### Analysis One:

This analysis focuses on issue activity by their state (Open vs. Closed), providing insights into the project's maintenance trends and potential backlogs. The feature can be run using:

```bash
python run.py --feature 1
```

#### Key Highlights

- **Categorization**: Differentiates issues into Open and Closed states for better insights.
- **Visualization**: Outputs a pie chart to visualize the distribution of Open and Closed issues.
- **Modular Design**: Built using the `IssueStateAnalysis` class for flexibility and reusability.
- **Workflow Integration**: Seamlessly added via the `--feature 1` flag in the CLI.

#### Benefits

- **Enhanced Insights**: Quickly identify backlogs and unresolved issues.
- **Actionable Focus**: Visual summaries make decision-making easier.
- **Future-Ready**: Scalable design for additional analysis and customizations.

### Analysis Two:

The second feature provides insights on the issues based off of their labels. The basic capabilities of the feature can be ran with the following command:
```
python run.py --feature 2
```
Additional insights can be generated by passing a label as an argument, as follows:
```
python run.py --feature 2 --label LABEL_NAME
```
where `LABEL_NAME` is the name of the label (e.g. `"Bug"` or `"New Feature"`).

The feature provides the following analytics.
- Simple Label Analysis
  - Calculates the total number of labels amongst all issues
  - Finds the number of issues with the user inputted label (all by default)
- Plot of Top Labels
  - Plots the top 20 most used labels and the number of issues with those labels
- Simple Unlabeling Analysis
  - Calculates the average number of unlabeling events per issue
  - Plots the number of issues by the number of unlabeling events in those issues
- Plot of New Issues Over Time For Label
  - Plots the number of new issues created with a user inputted label by time (does nothing if no label is input)

### Analysis Three:

The third feature aims to analyze the issue lifecycle: Created → Closed → Reopened. The timestamp for these events are used to plot various graphs which draw interesting insights:
```
python run.py --feature 3
```

The feature provides following:
- Issue lifecycle analysis
  - Plots a gantt chart to provide the entire lifecycle of an Issue from its creation to closing till reopening.
  - Facilitates comparative analysis of multiple issues to identify patterns and bottlenecks.
- Issue's reopening trends
  - Plots a bar graph to display issue-reopening trends over time.
  - Helps understand:
    - Peaks in issue reopenings.
    - Seasonal or periodic patterns in issue reoccurrence.
- Time taken to reopen analysis
  - Plots a donut chart which reveals insights into the speed of issue reopening after closure.
  - Helps to identify whether issues are being addressed promptly or if delays exist in their resolution.

## Testing

Each feature has its respective test cases, which can be found on the `test` branch or by viewing the PR for said branch. The results of our findings are as follows.

### Analysis One:

### Analysis Two:

The test file for this feature can be found on the `test` branch and located at `~/tests/test_label_analysis.py`. The main bug that was found with the functions in feature 2 was that there was no type checking. This would lead to the program crashing if an invalid type was passed as an argument, such as `None`, an empty list, etc. As a part of the `test` branch, these type checks were added to `label_analysis.py` so that the bugs are resolved & the assertions pass. Additionally, unit tests are performed on each of the functions with basic data to ensure that the functions return what they should.

### Analysis Three:

#### Test File and Bug Fix for Feature 3

The test file for this feature can be found on the `test` branch at `~/tests/test_issue_lifecycle_analysis.py`.

During testing, a bug was identified in the methods within Feature 3. Specifically, the issue_lifecycle_analysis.py file methods were missing the `self` argument, which led to the following error when tests were run:  
```
TypeError: IssueLifecycleAnalysis.plot_lifecycle() takes 0 positional arguments but 1 was given  
```
This error typically occurs when a method that is supposed to be an instance method (i.e., a method that belongs to a class) is defined without the `self` parameter. The issue was resolved by adding `self` as an argument in the method definitions and calling. This fix eliminated the error, and the tests no longer fail. 

#### Unit Tests
The following unit tests were written to ensure proper functionality:

- test_configuration_initialization: Verifies that the class correctly initializes its configuration parameters using mock data.
- test_plot_lifecycle: Ensures the plotting functions (plot_gantt_chart, plot_reopening_trend, and plot_reopened_issue_timing) are called correctly with data for a single issue having both 'reopened' and 'closed' events.
- test_multiple_issues: Confirms the correct handling of multiple issues, ensuring that gantt_data and reopened_issues_list are feeded properly for plotting.
- test_issues_without_reopened_events: Validates that issues without 'reopened' events are excluded from plotting, ensuring all related functions handle such cases gracefully.

These tests help verify the correctness of the functionality and ensure that the feature works as expected under different scenarios.
